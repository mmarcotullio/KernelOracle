# -*- coding: utf-8 -*-
"""preprocess.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YfoXq7l_OeP8iwxt_vPcjHMNdi279lfO
"""

from __future__ import annotations

import argparse
import json
import os
from typing import Dict, Tuple

import numpy as np
import pandas as pd


def build_vocab(df: pd.DataFrame) -> Dict:

    pids = df["pid"].astype(str).unique().tolist()
    pids_sorted = sorted(pids)
    pid_to_idx = {p: i for i, p in enumerate(pids_sorted)}
    idx_to_pid = {i: p for p, i in pid_to_idx.items()}


    states = df["prev_state"].astype(str).fillna("UNK").unique().tolist()
    states_sorted = sorted(states)
    state_to_idx = {s: i for i, s in enumerate(states_sorted)}
    idx_to_state = {i: s for s, i in state_to_idx.items()}

    return {
        "pid_to_idx": pid_to_idx,
        "idx_to_pid": idx_to_pid,
        "state_to_idx": state_to_idx,
        "idx_to_state": idx_to_state,
    }


def encode_df(df: pd.DataFrame, vocab: Dict) -> pd.DataFrame:
    df = df.copy()

    df["pid_str"] = df["pid"].astype(str)
    df["pid_idx"] = df["pid_str"].map(vocab["pid_to_idx"]).astype(np.int64)

    df["prev_state_str"] = df["prev_state"].astype(str).fillna("UNK")

    if "UNK" not in vocab["state_to_idx"]:

        vocab["state_to_idx"]["UNK"] = len(vocab["state_to_idx"])
        vocab["idx_to_state"][vocab["state_to_idx"]["UNK"]] = "UNK"

    df["state_idx"] = df["prev_state_str"].map(lambda s: vocab["state_to_idx"].get(s, vocab["state_to_idx"]["UNK"])).astype(np.int64)


    df["cpu"] = pd.to_numeric(df.get("cpu", 0), errors="coerce").fillna(0.0).astype(np.float32)
    df["time_diff"] = pd.to_numeric(df.get("time_diff", 0), errors="coerce").fillna(0.0).astype(np.float32)


    df["timestamp"] = pd.to_numeric(df["timestamp"], errors="coerce")
    df = df.dropna(subset=["timestamp"]).sort_values("timestamp").reset_index(drop=True)

    return df


def make_windows(df: pd.DataFrame, seq_len: int, stride: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:

    pid = df["pid_idx"].to_numpy(np.int64)
    state = df["state_idx"].to_numpy(np.int64)
    cpu = df["cpu"].to_numpy(np.float32)
    td = df["time_diff"].to_numpy(np.float32)

    cont = np.stack([cpu, td], axis=-1).astype(np.float32)  # (T, 2)

    T = len(df)
    max_start = T - (seq_len + 1)
    if max_start <= 0:
        raise ValueError(f"Not enough rows ({T}) for seq_len={seq_len} (+1 target). Need at least {seq_len+1}.")

    X_pid = []
    X_state = []
    X_cont = []
    y = []

    for start in range(0, max_start + 1, stride):
        end = start + seq_len
        X_pid.append(pid[start:end])
        X_state.append(state[start:end])
        X_cont.append(cont[start:end, :])
        y.append(pid[end])

    X_pid = np.stack(X_pid, axis=0)
    X_state = np.stack(X_state, axis=0)
    X_cont = np.stack(X_cont, axis=0)
    y = np.array(y, dtype=np.int64)

    return X_pid, X_cont, X_state, y


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--csv", required=True, help="Path to a CSV trace (e.g., data_tcn/train.csv)")
    ap.add_argument("--out", required=True, help="Output npz path (e.g., tcn/artifacts/train.npz)")
    ap.add_argument("--vocab_out", default="tcn/artifacts/vocab.json", help="Where to save vocab.json")
    ap.add_argument("--seq_len", type=int, default=64)
    ap.add_argument("--stride", type=int, default=1)
    ap.add_argument("--use_existing_vocab", action="store_true",
                    help="If set, load vocab_out and do NOT rebuild (use for test splits).")
    args = ap.parse_args()

    os.makedirs(os.path.dirname(args.out), exist_ok=True)
    os.makedirs(os.path.dirname(args.vocab_out), exist_ok=True)

    df = pd.read_csv(args.csv)

    required = ["timestamp", "pid", "cpu", "time_diff", "prev_state"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"Missing required columns: {missing}. Found: {list(df.columns)}")


    if args.use_existing_vocab and os.path.exists(args.vocab_out):
        with open(args.vocab_out, "r") as f:
            vocab = json.load(f)
    else:
        vocab = build_vocab(df)
        with open(args.vocab_out, "w") as f:
            json.dump(vocab, f)

    df_enc = encode_df(df, vocab)
    X_pid, X_cont, X_state, y = make_windows(df_enc, seq_len=args.seq_len, stride=args.stride)

    np.savez_compressed(
        args.out,
        pid=X_pid,
        cont=X_cont,
        state=X_state,
        y=y,
        seq_len=np.array([args.seq_len], dtype=np.int64),
    )

    print(f"Saved {args.out}")
    print(f"Windows: {X_pid.shape[0]}, seq_len={X_pid.shape[1]}, cont_dim={X_cont.shape[2]}")
    print(f"Num pids: {len(vocab['pid_to_idx'])}, Num states: {len(vocab['state_to_idx'])}")
    print(f"Vocab at: {args.vocab_out}")


if __name__ == "__main__":
    main()