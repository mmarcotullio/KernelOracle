# -*- coding: utf-8 -*-
"""tcn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YfoXq7l_OeP8iwxt_vPcjHMNdi279lfO
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Optional

import torch
import torch.nn as nn
import torch.nn.functional as F


class CausalConv1d(nn.Module):

    def __init__(self, in_ch: int, out_ch: int, kernel_size: int, dilation: int, dropout: float):
        super().__init__()
        self.kernel_size = kernel_size
        self.dilation = dilation
        self.conv = nn.Conv1d(
            in_channels=in_ch,
            out_channels=out_ch,
            kernel_size=kernel_size,
            dilation=dilation
        )
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (B, C, L)
        pad_left = (self.kernel_size - 1) * self.dilation
        x = F.pad(x, (pad_left, 0))
        y = self.conv(x)
        y = self.dropout(y)
        return y


class ResidualBlock(nn.Module):
    def __init__(self, channels: int, kernel_size: int, dilation: int, dropout: float):
        super().__init__()
        self.conv1 = CausalConv1d(channels, channels, kernel_size, dilation, dropout)
        self.conv2 = CausalConv1d(channels, channels, kernel_size, dilation, dropout)
        self.norm1 = nn.BatchNorm1d(channels)
        self.norm2 = nn.BatchNorm1d(channels)

    def forward(self, x: torch.Tensor) -> torch.Tensor:

        y = self.conv1(x)
        y = self.norm1(y)
        y = F.relu(y)
        y = self.conv2(y)
        y = self.norm2(y)
        y = F.relu(y)
        return x + y


@dataclass
class TCNConfig:
    num_pids: int
    num_states: int
    pid_emb: int = 32
    state_emb: int = 8
    cont_dim: int = 2
    channels: int = 128
    kernel_size: int = 3
    num_blocks: int = 6
    dropout: float = 0.1


class TCNNextPid(nn.Module):

    def __init__(self, cfg: TCNConfig):
        super().__init__()
        self.cfg = cfg

        self.pid_emb = nn.Embedding(cfg.num_pids, cfg.pid_emb)
        self.state_emb = nn.Embedding(cfg.num_states, cfg.state_emb)

        in_feat = cfg.pid_emb + cfg.state_emb + cfg.cont_dim
        self.proj = nn.Linear(in_feat, cfg.channels)

        blocks = []
        for i in range(cfg.num_blocks):
            dilation = 2 ** i
            blocks.append(ResidualBlock(cfg.channels, cfg.kernel_size, dilation, cfg.dropout))
        self.tcn = nn.Sequential(*blocks)

        self.head = nn.Linear(cfg.channels, cfg.num_pids)

    def forward(self, pid: torch.Tensor, cont: torch.Tensor, state: Optional[torch.Tensor] = None) -> torch.Tensor:

        if state is None:
            raise ValueError("state tensor is required")

        pid_e = self.pid_emb(pid)
        st_e = self.state_emb(state)

        x = torch.cat([pid_e, st_e, cont], dim=-1)
        x = self.proj(x)


        x = x.transpose(1, 2)
        x = self.tcn(x)


        last = x[:, :, -1]
        logits = self.head(last)
        return logits