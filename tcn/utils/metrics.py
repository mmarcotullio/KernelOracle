# -*- coding: utf-8 -*-
"""metrics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YfoXq7l_OeP8iwxt_vPcjHMNdi279lfO
"""

from __future__ import annotations
import time
from typing import Dict
import torch


@torch.no_grad()
def top1_accuracy(logits: torch.Tensor, targets: torch.Tensor) -> float:

    preds = torch.argmax(logits, dim=-1)
    correct = (preds == targets).sum().item()
    return correct / max(1, targets.numel())


@torch.no_grad()
def measure_inference_latency_ms(model: torch.nn.Module,
                                 batch: Dict[str, torch.Tensor],
                                 warmup: int = 20,
                                 iters: int = 100) -> float:

    model.eval()

    def _sync():
        if torch.cuda.is_available():
            torch.cuda.synchronize()

    # Warmup
    for _ in range(warmup):
        _ = model(batch["pid"], batch["cont"])

    _sync()
    t0 = time.perf_counter()
    for _ in range(iters):
        _ = model(batch["pid"], batch["cont"])
    _sync()
    t1 = time.perf_counter()

    avg_ms = (t1 - t0) * 1000.0 / iters
    return avg_ms